{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b63edd4",
      "metadata": {},
      "outputs": [],
      "source": [
        "from data_utils import *\n",
        "from model_utils import *\n",
        "from train_utils import *\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2bdfad2",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Data Path being used: {DATA_PATH}\")\n",
        "dataset = load_dataset_for_lstm()\n",
        "classes = dataset.classes\n",
        "\n",
        "train_dataset, val_dataset = split_dataset(dataset, val_ratio=0.2, random_seed=42)\n",
        "train_loader, val_loader = get_loaders(train_dataset, val_dataset, batch_size=32)\n",
        "\n",
        "print(f\"Aantal classes: {len(classes)}\")\n",
        "print(f\"Classes: {classes}\")\n",
        "print(f\"Training sequences: {len(train_dataset)}\")\n",
        "print(f\"Validation sequences: {len(val_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68f3f61a",
      "metadata": {},
      "outputs": [],
      "source": [
        "in_dim = 258\n",
        "num_classes = len(classes)\n",
        "HIDDEN_SIZE = 128\n",
        "NUM_LAYERS = 2\n",
        "SEQUENCE_LENGTH_REF = SEQUENCE_LENGTH # Add for reference\n",
        "\n",
        "model = create_model(num_classes, in_dim, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS)\n",
        "\n",
        "try:\n",
        "    model.load_state_dict(torch.load(os.path.join(MODEL_DIR, 'lstm_model.pth'), map_location=DEVICE))\n",
        "    print(\"Model weights loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"No pre-trained weights found. Starting from scratch.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7aca298",
      "metadata": {},
      "outputs": [],
      "source": [
        "label_counts = {}\n",
        "for idx in dataset.y:\n",
        "    gesture_name = dataset.classes[idx]\n",
        "    label_counts[gesture_name] = label_counts.get(gesture_name, 0) + 1\n",
        "\n",
        "class_counts = pd.Series(label_counts).sort_index()\n",
        "\n",
        "plt.figure(figsize=(15, 6))\n",
        "class_counts.plot(kind='bar', color='steelblue')\n",
        "plt.title('Sequence Count Distributie in Dataset', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Class', fontsize=12)\n",
        "plt.ylabel('Aantal Sequences', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nTotaal aantal sequences: {len(dataset)}\")\n",
        "print(f\"Gemiddeld aantal sequences per class: {class_counts.mean():.1f}\")\n",
        "print(f\"Min sequences: {class_counts.min()}, Max sequences: {class_counts.max()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94fab673",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Collect all predictions, labels, and confidence scores\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "all_confidences = []\n",
        "correct_confidences = []\n",
        "incorrect_confidences = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for inputs, labels, lengths in val_loader:\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        \n",
        "        # Forward pass (LSTM requires inputs AND lengths)\n",
        "        outputs = model(inputs, lengths) \n",
        "        \n",
        "        # Get probabilities using Softmax\n",
        "        probs = torch.softmax(outputs, dim=1)\n",
        "        \n",
        "        # Get the max probability (confidence) and the predicted class\n",
        "        confidences, predicted = torch.max(probs.data, 1)\n",
        "        \n",
        "        # Store results\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "        \n",
        "        # Separate confidences into correct and incorrect for analysis later\n",
        "        for conf, pred, label in zip(confidences.cpu().numpy(), \n",
        "                                     predicted.cpu().numpy(), \n",
        "                                     labels.numpy()):\n",
        "            all_confidences.append(conf)\n",
        "            if pred == label:\n",
        "                correct_confidences.append(conf)\n",
        "            else:\n",
        "                incorrect_confidences.append(conf)\n",
        "\n",
        "all_preds = np.array(all_preds)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "# Calculate overall accuracy\n",
        "accuracy = 100 * (all_preds == all_labels).sum() / len(all_labels)\n",
        "print(f\"Validation Accuracy: {accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92b12e4d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Plot Confusion Matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=classes, yticklabels=classes,\n",
        "            cbar_kws={'label': 'Aantal voorspellingen'})\n",
        "plt.title('Confusion Matrix (LSTM Model)', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.ylabel('Werkelijke Class', fontsize=12)\n",
        "plt.xlabel('Voorspelde Class', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b06d0aab",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Print Classification Report\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CLASSIFICATION REPORT\")\n",
        "print(\"=\"*80)\n",
        "print(classification_report(all_labels, all_preds, target_names=classes, digits=3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f82843a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Calculate and Plot Per-Class Accuracy\n",
        "per_class_accuracy = []\n",
        "for i in range(len(classes)):\n",
        "    mask = all_labels == i\n",
        "    if mask.sum() > 0:\n",
        "        acc = 100 * (all_preds[mask] == all_labels[mask]).sum() / mask.sum()\n",
        "        per_class_accuracy.append(acc)\n",
        "    else:\n",
        "        per_class_accuracy.append(0)\n",
        "\n",
        "plt.figure(figsize=(15, 6))\n",
        "# Color code: Green > 90%, Orange > 70%, Red < 70%\n",
        "colors = ['green' if acc >= 90 else 'orange' if acc >= 70 else 'red' for acc in per_class_accuracy]\n",
        "\n",
        "plt.bar(classes, per_class_accuracy, color=colors, alpha=0.7)\n",
        "plt.axhline(y=accuracy, color='blue', linestyle='--', label=f'Overall Accuracy: {accuracy:.2f}%', linewidth=2)\n",
        "plt.title('Per-Class Accuracy', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Class', fontsize=12)\n",
        "plt.ylabel('Accuracy (%)', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.ylim([0, 105])\n",
        "plt.legend()\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print classes with lowest accuracy\n",
        "print(\"\\nClasses met laagste accuracy:\")\n",
        "sorted_indices = np.argsort(per_class_accuracy)[:5]\n",
        "for idx in sorted_indices:\n",
        "    print(f\"  {classes[idx]}: {per_class_accuracy[idx]:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d619600",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Plot Confidence Distribution\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Histogram\n",
        "plt.subplot(1, 2, 1)\n",
        "if len(correct_confidences) > 0:\n",
        "    plt.hist(correct_confidences, bins=20, alpha=0.7, label='Correct', color='green', edgecolor='black')\n",
        "if len(incorrect_confidences) > 0:\n",
        "    plt.hist(incorrect_confidences, bins=20, alpha=0.7, label='Incorrect', color='red', edgecolor='black')\n",
        "plt.xlabel('Confidence Score', fontsize=12)\n",
        "plt.ylabel('Aantal Voorspellingen', fontsize=12)\n",
        "plt.title('Model Confidence Distributie', fontsize=14, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "# Boxplot\n",
        "plt.subplot(1, 2, 2)\n",
        "data_to_plot = []\n",
        "labels_to_plot = []\n",
        "\n",
        "if len(correct_confidences) > 0:\n",
        "    data_to_plot.append(correct_confidences)\n",
        "    labels_to_plot.append('Correct')\n",
        "if len(incorrect_confidences) > 0:\n",
        "    data_to_plot.append(incorrect_confidences)\n",
        "    labels_to_plot.append('Incorrect')\n",
        "\n",
        "if data_to_plot:\n",
        "    plt.boxplot(data_to_plot, labels=labels_to_plot)\n",
        "    plt.ylabel('Confidence Score', fontsize=12)\n",
        "    plt.title('Confidence Score Vergelijking', fontsize=14, fontweight='bold')\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "if len(correct_confidences) > 0:\n",
        "    print(f\"\\nGemiddelde confidence (correct): {np.mean(correct_confidences):.4f}\")\n",
        "if len(incorrect_confidences) > 0:\n",
        "    print(f\"Gemiddelde confidence (incorrect): {np.mean(incorrect_confidences):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd51fc59",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Model Architecture and Parameters\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL ARCHITECTURE\")\n",
        "print(\"=\"*80)\n",
        "print(model)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\nTotaal aantal parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Input dimensie: {in_dim}\")\n",
        "print(f\"Hidden Size: {HIDDEN_SIZE}\")\n",
        "print(f\"Sequence Length (Ref): {SEQUENCE_LENGTH_REF}\")\n",
        "print(f\"Output classes: {num_classes}\")\n",
        "print(f\"Device: {DEVICE}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
